---
title: "DATA 621 HW 3 Crime"
author: "Jian Quan Chen, Frederick Jones"
date: "2024-03-22"
output:
  pdf_document: default
  html_document: default
---

# 1. Data Exploration

```{r warning=FALSE}
library(tidyverse)
library(psych)
library(corrplot)
library(MASS)
```

```{r}
df <- read.csv("https://raw.githubusercontent.com/LeJQC/DATA-621-Group-2/main/HW3/crime-training-data_modified.csv")
```

Let's get a general sense of the training data set using the `glimpse` function. There are 466 observations and 13 variables in the training data set. Of the 13 variables, 12 are predictor variables and 1 is the target variable. All of the variables seem to be floats or integers.

```{r}
glimpse(df)
```

The variables are:

-   `zn`: proportion of residential land zoned for large lots (over 25000 square feet)
-   `indus`: proportion of non-retail business acres per suburb
-   `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0)
-   `nox`: nitrogen oxides concentration (parts per 10 million)
-   `rm`: average number of rooms per dwelling
-   `age`: proportion of owner-occupied units built prior to 1940
-   `dis`: weighted mean of distances to five Boston employment centers
-   `rad`: index of accessibility to radial highways
-   `tax`: full-value property-tax rate per \$10,000
-   `ptratio`: pupil-teacher ratio by town
-   `lstat`: lower status of the population (percent)
-   `medv`: median value of owner-occupied homes in \$1000s
-   `target`: whether the crime rate is above the median crime rate (1) or not (0)

## Summary Table
Using the `describe` function from the psych library, we can get the summary statistics of all the variables as shown in the table below. Something to note here is the large standard deviation with the `zn`, `age` variable compared to its range, which may be because they are proportions. Another thing that stands out is the `tax` variable. The mean, standard deviation, median, and range are much larger than the other predictor variables so we might have to do some transformation on it later on. 
```{r}
summary_table <- describe(df)

print(round(summary_table,2))
```

## Distrubtion Plot
Next, let's look at the distribution of the 13 variables. As expected the values of the target variable are distributed around 0 and 1. Variables such as age, chas, dis, lstat, nox, ptration, rad, and zn are skewed to the left or right. On the other hand, variables medv and rm are normally distributed. The remaining predictor variables indus and tax have a bimodal distribution curve. 
```{r}
df_long <- df %>%
  pivot_longer(
    cols = everything(), 
    names_to = "variable",
    values_to = "value"
  )

df_long %>%
  ggplot(aes(value)) + 
  geom_density(fill = "blue") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

## Correlation Plot
To identify the correlation between each variable we can create a correlation plot by using the corrplot library. According to the correlation plot, the variables nox (0.73), age (0.63), rad (0.63), tax (0.61), and indus (0.60) are most linearly correlated with the target variable. In addition, there is a high degree of collinearity with tax and rad (0.91) and with nox and indus (0.76). These are variables to keep in mind when we select variables for our models. 
```{r}
df %>% 
  cor(use = "complete.obs") %>%
  corrplot(method = "color", tl.col = "black", addCoef.col = "black", number.cex = 0.5)
```

## Boxplot
This boxplot shows the distribution and quartile ranges of the variables. We see that there are obvious outliers in chas and dis, which we may have to deal with later on. Other predictor variables such as lstat, medv, rm, and zn also have outliers, which may need to be dealth with later as well. 
```{r}
df_long %>% 
  ggplot(aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(~variable, scales='free', ncol=5)
```

# 2. Data Preparation

## Fix any missing value in the data
**Let's check if there exists a null value or NA in the dataframe. If Null is found it can be fixed using mean or median of the column in which na is found.**
**Attach the data**
```{r}
training_set <- df
test_set <- read.csv("https://raw.githubusercontent.com/LeJQC/DATA-621-Group-2/14f8d86b73472ac8f9302415a7e4c24b37052ea6/HW3/crime-evaluation-data_modified.csv")

```

```{r}
print(sum(is.na(training_set)))
print(sum(is.na(test_set)))
```
This shows that there are no missing values in the data and thus no need to bother about the missing values. 

## Modify an existing variable or create a new variable by combining the two or more variables 
It is to be noted that the tax column contain the tax rate per \$10,000 and medv contain the property value in $1000. The median tax on properties in neighborhood can be calculated by using these two columns. 
$$medTax = \frac{medv*1000}{10000}*Tax$$
or in simple it can be written as medTax = medv*Tax*1000/10000 = medv*tax/10

```{r}
training_set<- training_set|>mutate(
  medTax = medv*tax/10
)|>
  relocate(medTax, .before = 10)
test_set<- training_set|>mutate(
  medTax = medv*tax/10
)|>relocate(medTax, .before = 10)
head(training_set)
  
```
Now no need of the variable tax as we have median value of tax per owner occupied home.Drop the variable tax from the data.
```{r}
training_set<- training_set[,-9]
test_set<- test_set[,-9]
#head(training_set)
#head(test_set)
```

## Feature scaling

**Now the data can be scaled so that the all the predictors have equal effect on the response variable.**

It can concluded from the summary table that the range of some variable is high like zn's range is 0-100. This range can be normalize or standardize so that the range is between  0 and 1 or -3 to 3 respectively. 

To normalize a number we just divide it by the maximum value in the variable. This can be done conveniently using scale() function in R. 

Note: No need to standardize the target variable, the predictor variables chas and nox because these already contain 0 and 1 or within the standardized limit.    
```{r}
training_set[,1:12][,c(-3, -4)] = scale(training_set[, 1:12][,c(-3, -4)])
test_set = scale(test_set)
head(training_set)
```

# 3. Build Models (Logistic regression binary models)

## Model 1
The first task before writing the any model is to select the predictor variables.But the selection of suitable predictor variable is a complex task and if we omit any predictor variable which influence the target variable, it induces a bias in the output of the model which is known as omitted-variable bias. Thus, one needs to be very careful while selecting a suitable predictor variable. 

We can use the most correlated variables with the target variable. Using the scatterplot, it can be seen that the target variable is highly correlated with the following variables: 

indus, nox, age, rad, lstat 

And weakly correlated with medTax, ptratio

And almost neutral with chas and highly negatively correlated with zn, dis, medv, rm.

The correlation plot can be regenerated on the modified or transformed data to see the correlation among the transformed variables. 
```{r}
training_set %>% 
  cor(use = "complete.obs") %>%
  corrplot(method = "color", tl.col = "black", addCoef.col = "black", number.cex = 0.5)
```
The highly correlated components can be selected for the logistic regression. Therefore, we will use indus, nox, age, rad, lstat, zn, dis. It can be seen that zn is higly correlated with nox, indus, age, dis,ptratio, lstate, medv. Therefore, it will be better to leave zn and no need to include in in the regression. 

```{r}
logistic_model <-  glm (target ~ indus + nox + age + rad + lstat + dis,data = training_set, family = binomial)
summary(logistic_model)
```
The p-value for the lstat is 0.4999 which is more than the critical value 0.05 thus this variable must be omitted from the regression. 

```{r}
logistic_model <-  glm(target ~ indus + nox + age + rad + dis,data = training_set, family = binomial)
summary(logistic_model)
```
All the probability of the coefficients are less than 0.05 thus, it can be concluded that all the coefficients are significant for the regression. 
**Test the hypothesis with chisq-test**
```{r}
anova(logistic_model, test="Chisq")
```
The probability of dis is more than 0.05 thus the variable dis can be removed from the model and we have the logistic model. 
```{r}
logistic_model <-  glm(target ~ indus + nox + age + rad, data = training_set, family = binomial)
anova(logistic_model, test="Chisq")
```
Now the variable age has pr(>Chi)=0.2333>0.05, thus age seems to be removable, so let's drop it from the logistic model. 

```{r}
logistic_model <-  glm(target ~ indus + nox + rad, data = training_set, family = binomial)
anova(logistic_model, test="Chisq")
```
Now the all the variables have probability less than 0.05 and thus these three variables are significant for the logistic binary regression. 
**Prediction of the based on the final model**
```{r}
pred_target <- predict(logistic_model, type="response", data=test_set)
pred_target <- ifelse(pred_target > 0.5, 1, 0)
pred_target[1:10]
```
So the prediction 

## Model 2
For this binary logistic regression model, we are going to use all the predictor variables from the original training data set. Then we are going to use stepwise selection to remove predictors based on Akaike Information Criterion (AIC) to create an optimal model. This will be done using the `stepAIC` function from the MASS library, which will iterate over the predictors to improve the model's AIC.   

```{r}
logistic_model2 <- glm(target ~., family="binomial", data=df)
summary(logistic_model2)
``` 
From this initial model with all the predictors, we can see that there are several variables that are not statistically significant such as indus, chas, rm, and lstat. This model has an AIC of 218.05. Next, lets use the stepAIC function to improve this model. 
```{r}
step_model <- stepAIC(logistic_model2, direction = "both", trace = FALSE)
summary(step_model)
```
In this new model, the predictor variables that were not statistically significant were removed. The AIC for this new model is 215.32, which is slightly lower than the initial model (218.05), indicating that removing those predictor variables improved the fit of the model.

The intercept or log-odds when predictors are zero is -37.4. As for coefficients, a one unit increase in nox(nitrogen oxide concentration) corresponds to an increase of 42.8 in the log-odds of high crime. A one unit increase in age, dis, rad, ptratio, and medv corresponds to an increase of 0.03, 0.65, 0.72, 0.32, and 0.11 in the log-odds of high crime, respectively. While a one unit increase in zn and tax results in a decrease of -0.06 and -0.007 in the log-odds of high crime. Out of all the predictors, it is strange to think that nitrogen oxides concentration had the largest influence on crime rate. How can air pollution be linked to crime? I can see the possibility that an increase in nitrogen oxide concentrations affecting physical and mental health, which can influence criminal behavior. However, this seems like a stretch and it is hard to see a correlation between the nitrogen oxide concentrations and crime rate intuitively. As for zn and tax, the two variables that corresponded to a decrease in crime, it seems practical that a crammed area with low property value will have a higher rate of crime.

Although there are predictors in the model do not make sense intuitively, all the predictors are statistically significant. Also, based on AIC, this model is a better fit for the target variable than the initial model with all the predictors.


## MOdel 3

The variables I chose for this model were picked because they seem like they could have a negative impact on crime rates in a neighborhood. The proportion of residential land zoned for large lots (zn) might say something about the neighborhood's layout and size, which could affect how much crime happens there. The weighted mean of distances to Boston employment centers (dis) could show how easy it is for people in the area to get to work. The pupil-teacher ratio by town (ptratio) might reflect how well-funded the schools are, which could be linked to the overall quality of the neighborhood. The median value of owner-occupied homes in $1000s (medv) could tell us about the wealth and stability of the people living there, which could also influence crime rates.

```{r}
logistic_model_3 <- glm(target ~ zn + dis + ptratio + medv, data = training_set, family = binomial)

summary(logistic_model_3)
```

In our journey to predict property sales, we started with a model that considered every possible factor. We found that some things, like being close to job centers and air quality, really mattered, while others, like land use type and the number of rooms, didn't seem to make much of a difference. Then, we decided to simplify things. We focused on just a few key predictors: how much of the area was residential, how far it was from work, the pupil-teacher ratio, and the median home value. What surprised us was that even with fewer factors, our predictions stayed pretty accurate. The numbers tell the story too: the first model had an AIC of 218.05, while the second was at 406.93. Based on the AIC (AIC: 218.05 for model 2, AIC: 406.93 for model 3) and residual deviance metrics (Residual Deviance: 192.05 for model 2, Residual Deviance: 396.93 for model 3), it appears that model 2 is indeed performing better than model 3. The lower AIC and residual deviance values of model 2 suggest that it provides a better fit to the data and likely has higher predictive accuracy.



Now, leveraging this logistic model, we'll employ stepwise variable selection to refine our model's predictive power and enhance its interpretability.
```{r}
step_model3 <- stepAIC(logistic_model_3, direction = "both", trace = FALSE)
summary(step_model3)
```


Comparing the residual deviance values, the stepwise model with residential land proportion, distance to employment centers, and pupil-teacher ratio (step_model3) exhibits a residual deviance of 397.46. This is higher than the residual deviance of 197.32 for the previous stepwise model with additional variables (step_model). Despite the reduction in variables, the residual deviance of step_model3 is higher, suggesting that it may not fit the data as well as the previous stepwise model. Also, the AIC of 405.46 for step_model3 is  higher than the AIC of 215.32 for step_model, indicating that step_model3does not provide a better fit to the data.



