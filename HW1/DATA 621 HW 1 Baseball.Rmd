---
title: "DATA 621 HW 1"
author: "Group 2 - Tilon Bobb, Jian Quan Chen, Shamecca Marshall"
date: "2024-02-22"
output:
  pdf_document: default
  html_document: default
---

```{r include=FALSE, }
knitr::opts_chunk$set(echo=FALSE,warning = FALSE, message = FALSE)
```

# Introduction

In this assignment, we are given a baseball training and evaluation dataset, which contains approximately 2200 records. The data spans from 1871 to 2006, with each row representing a baseball team's performance from that year. The statistics were all adjusted to reflect a 162 game season. Our objective is to construct a multiple linear regression model of the training data to predict the number of wins for a team.

# 1. Data Exploration

```{r warning=FALSE}
library(tidyverse)
library(psych)
library(corrplot)
```

```{r}
df <- read_csv("https://raw.githubusercontent.com/LeJQC/DATA-621-Group-2/main/HW1/moneyball-training-data.csv", show_col_types = FALSE)
```

### Glimpse of the data

There are 2,276 rows and 17 columns in the dataset. The response variable is `TARGET_WINS` and the remaining 15 variables, with the exception of the `INDEX` column, are predictor variables.

```{r warning=FALSE}
glimpse(df)
```

### Summary table

We can see from the summary, that the mean of `TARGET_WINS` is 80.79, which is about half the games in a baseball season. For the most part, most variables have 2276 values but there are some, `TEAM_BATTING_HBP` in particular, have less, suggesting that there is missing data. 

```{r warning=FALSE}
# Setting index column to index
rownames(df) <- df$INDEX
df$INDEX <- NULL

# Print summary table
summary_table <- describe(df)

print(round(summary_table,2))
```

### Distribution of variables

The `TARGET_WINS`, `TEAM_BATTING_2B`, `TEAM_BATTING_HBP`, and `TEAM_FIELDING_DP` variables show a normal distribution. The `TEAM_BATTING_HR`, `TEAM_BATTING_SO`, and `TEAM_PITCHING_HR` variables show a bimodal distribution.

```{r warning=FALSE}
df_long <- df %>%
  pivot_longer(
    cols = everything(), 
    names_to = "variable",
    values_to = "value"
  )

df_long %>%
  ggplot(aes(value)) + 
  geom_density(fill = "blue") + 
  facet_wrap(~variable, scales ="free", ncol = 4) +
  labs(x = element_blank(), y = element_blank())
```

### Correlation of variables

From this visual, wins seem to be most linearly correlated with `TEAM_BATTING_H` (0.39), `TEAM_BATTING_2B` (0.29), `TEAM_BATTING_BB` (0.23), `TEAM_PITCHING_HR` (0.19), and `TEAM_BATTING_HR` (0.18).

```{r}
df %>% 
  cor(use = "pairwise.complete.obs") %>%
  corrplot(method = "color", type = "upper", tl.col = "black", diag = TRUE, number.cex = 0.5, addCoef.col = 'black', tl.srt = 50, col=colorRampPalette(c("#9c89b8","#f0a6ca","#b8bedd"))(200))
```

# 2. Data Preparation

Checking for missing values within the dataset by creating flags for every column
```{r}
# Loop through columns
for (col_name in names(df)) {
  missing <- is.na(df[[col_name]])
  output <- paste(col_name,"missing values?",any(missing))
  print(output)
}
```
```{r}
# Checking for any missing values
sapply(df, function(x) sum(is.na(x)))
```

Since the `TEAM_BATTING_HBP` variable was missing 2000 values, we will just remove this column from the dataset. The other columns that had missing values (`TEAM_BATTING_SO`, `TEAM_BASERUN_SB`, `TEAM_BASERUN_CS`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_DP`) will be replaced with the median value of that variable.

```{r}
df <- df %>% select(-TEAM_BATTING_HBP)

na_variables <- c("TEAM_BATTING_SO", "TEAM_BASERUN_SB", "TEAM_BASERUN_CS", "TEAM_PITCHING_SO", "TEAM_FIELDING_DP")

for (col in na_variables) {
  median_value <- median(df[[col]], na.rm = TRUE)
  df[[col]][is.na(df[[col]])] <- median_value
}

summary(df)
```

Checking for any missing values within the dataset.
```{r}
#which(is.na(df))
 
# Count total missing values 
print("Count of total missing values  ")
sum(is.na(df))
```
Now there is no more missing values within the dataset.

# 3. Build Models

### Model 1
For this model, we are going to use the three variables that were most linearly correlated to target wins: `TEAM_BATTING_H` (0.39), `TEAM_BATTING_2B` (0.29), `TEAM_BATTING_BB` (0.23).

```{r}
model1 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_BB, data = df)

summary(model1)
```

The coefficient estimate for `TEAM_BATTING_H` is 0.045218, which means that each hit by a batter will increase the `TARGET_WINS` by 0.045. In addition, the coefficient estimate for `TEAM_BATTING_BB` is 0.034136, which indicates that every walk by a batter increases the `TARGET_WINS` by 0.034. Both `TEAM_BATTING_H` and `TEAM_BATTING_BB` have a p-value less than 0.05 so they are statistically significant in predicting the number of wins. On the other hand,  `TEAM_BATTING_2B` has a coefficient of -0.04206 with a p-value of 0.603. The negative coefficient suggests that the more doubles a team has, the lower amount of wins. However, since the p-value is so high, we cannot say there is a relationship between the number of doubles and the number of wins. 

The residual standard error of 13.92 indicates that the typical difference between the observed and predicted number of wins is 14 wins. This error is relatively large considering the average wins is 80.79. Also, the Multiple R-squared value of 0.2196 indicates that about 22% of the variability in wins can be explained by this multiple linear regression model. Lastly, the p-value of the model is <2.2e-16, signifies that this model is statistically significant in predicting the number of wins.

In summary, while this model is statistically significant in predicting the number of wins, we would not use it as the R-squared is low at 0.22 and the standard error is relatively high. Although it makes sense that the increasing the number of hits (`TEAM_BATTING_H`) and walks(`TEAM_BATTING_BB`) increases the amount of wins, it is odd that increasing the amount of doubles decreases the total wins. 

### Model 2

This multiple linear regression model omits the intercept within the model because if the intercept in a regression model predicting baseball team wins is negative, it suggests that even when all independent variables are set to zero, the model predicts a negative number of wins. This negative prediction essentially indicates that the team is expected to have more losses than wins, which is not realistic or meaningful in the context of baseball. 
```{r}
##Fit the multiple linear regression model
model2 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_2B+0, data = df)

summary(model2)
```
The coefficients obtained from our multiple linear regression model shed light on the relationship between specific baseball metrics and the number of wins. For instance, the coefficient for "TEAM_BATTING_H" (Base Hits by batters) is approximately 0.044, indicating that for each additional base hit, we expect around 0.044 more wins, holding other variables constant. Similarly, the coefficient for "TEAM_BATTING_BB" (Walks allowed) is approximately 0.034, suggesting that each additional walk allowed by the pitching team is associated with around 0.034 more wins.

However, the coefficient for "TEAM_BATTING_2B" (Doubles by batters) is approximately -0.003, which is statistically insignificant (p-value = 0.679). This suggests that the number of doubles by batters may not have a significant effect on wins. This finding may appear counterintuitive, as one might expect teams with more doubles to win more games.The residual standard error of 13.92 indicates that the typical difference between the observed and predicted number of wins is 14 wins. This error is relatively large considering the average wins is 80.79. 

Despite this inconsistency, the overall model demonstrates a strong ability to explain win variance, with an adjusted R-squared value of 0.9714. This indicates that the model accounts for a significant portion of the variability in wins based on the included variables. Therefore, it may be advisable to retain the model for further analysis and refinement.


#### Residual Analysis - Model 2
```{r echo=FALSE, out.width='70%', fig.align='center'}
layout(matrix(c(1,2,3,4),2,2))
plot(model2)
```

*Residuals vs Fitted: The residuals are clustered around 60-100, suggesting that the assumption of linearity is not met.
*Scale-location: The data is not randomly dispersed around the horizontal line so the assumption of homoscedasticity is not met.
*Normal Q-Q: For the most part, the plot follows the normal line but there are some deviations at the tail.

Judging from the residual plots, this model might not be the best fit for predicting the response variable. 

### Model 3

Let's construct a multiple linear regression model with the response variable as TARGET_WINS and additional explanatory variables as TEAM_PITCHING_H, TEAM_PITCHING_HR, and TEAM_PITCHING_BB. This selection is based on their correlation coefficients with TARGET_WINS: TEAM_PITCHING_H (-0.10993705), TEAM_PITCHING_HR (0.18901373), and TEAM_PITCHING_BB (0.124174536). Despite TEAM_PITCHING_H having a negative correlation coefficient, indicating a potentially negative impact on wins, it's essential to consider its significance in the model along with the positive coefficients of TEAM_PITCHING_HR and TEAM_PITCHING_BB. By including these variables, we aim to capture the collective influence of pitching-related statistics on the number of wins in our dataset.
```{r}
model3 <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BATTING_2B +  TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB + 0, data = df)

summary(model3)
```
The coefficients obtained from our multiple linear regression model shed light on the relationship between specific baseball metrics and the number of wins. For instance, the coefficient for "TEAM_BATTING_H" (Base Hits by batters) suggests that each additional base hit is associated with around 0.044 more wins, holding other variables constant. Similarly, the coefficient for "TEAM_BATTING_BB" (Walks allowed) indicates that each additional walk allowed by the pitching team correlates with around 0.034 more wins.

However, the coefficient for "TEAM_BATTING_2B" (Doubles by batters) is statistically insignificant (p-value = 0.679), suggesting that the number of doubles by batters may not significantly impact wins. This finding may seem counterintuitive, as one might expect teams with more doubles to win more games.

Despite this inconsistency, the overall model demonstrates a strong ability to explain win variance, with an adjusted R-squared value of 0.9714. Therefore, it may be advisable to retain the model for further analysis and refinement.

#### Residual Analysis - Model 3
```{r echo=FALSE, out.width='70%', fig.align='center'}
layout(matrix(c(1,2,3,4),2,2))
plot(model3)
```

*Residuals vs Fitted: The residuals are clustered around 60-100, suggesting that the assumption of linearity is not met.
*Scale-location: The previous plot showed a more uniform distribution of residuals across fitted values, while the current plot exhibits a dip in residuals around the range of 60 to 100 fitted values, indicating potential heteroscedasticity or a distinct pattern of variability in that range.
*Normal Q-Q: For the most part, the plot follows the normal line but there are some deviations at the tail.
*Residuals vs Leverage: We see a concentration of points toward the left end of the x-axis in the plot which suggests the presence of influential data points with high leverage, indicating they have a significant impact on the regression model's coefficients.

### Model 4
For this model, we are going to create an interaction term by taking the product of `TEAM_BATTING_H`,`TEAM_BATTING_HR`, and `TEAM_BATTING_BB`. These were most linearly correlated with `TARGET_WINS` and all of these variables have a positive impact on the number of wins. An interaction term will show that the multiplicative effects of these variables on predicting the `TARGET_WINS` might be better than the sum of them individually. Doubles hit by batters was not included as it had a high p-value in the first model, suggesting it might not be a significant predictor, and it is slightly correlated with hits by batter. 

```{r}
model4 <- lm(TARGET_WINS ~ (TEAM_BATTING_H * TEAM_BATTING_BB* TEAM_BATTING_HR) + 0, data = df)

summary(model4)
```
These results of the coefficient estimates are a bit surprising as I was expecting all of them to be positive since they all have a positive impact on wins. The coefficients for `TEAM_BATTING_H` and `TEAM_BATTING_BB` are positive, indicating that an increase in these variables is associated with an increase in `TARGET_WINS`. However, the main effect of `TEAM_BATTING_HR` is negative, suggesting that an increase in home runs is associated with a decrease in wins. This is counterintuitive as one would expect the more home runs a team hit, the more wins they have. Also, the coefficient for the interaction term `TEAM_BATTING_H:TEAM_BATTING_BB:TEAM_BATTING_HR` is -4.995e-07, indicating that for each combined base hit, walk, and home run, th model predicts a decrease in the amount of wins by 4.995e-07. Again, this does not seem to align with the fact that these individually have a positive impact on wins. All of the p-values are low indicating statistical significance. However, coefficients themselves are small so it debatable how relative this model is in predicting wins.

Like the previous models, the R-squared is high at 0.9717 so 97% of the variance in the response model can be attributed to this model. In addition, all of the p-values for these variables are low, indicating that they are statistically significant predictors in this model. But, the magnitude of these coefficients is quite small, which suggests the practical significance of these predictors in this model. Overall, while this model has strong statistical significance, the coefficients do not match with our conception of baseball so it will need further refinement. 

#### Residual Analysis - Model 4
```{r echo=FALSE, out.width='70%', fig.align='center'}
layout(matrix(c(1,2,3,4),2,2))
plot(model4)
```
These residual plots are similar to the previous models. The plots suggest some issues with the assumptions of homoscedasticity and linearity. Therefore a linear regression model may not be the best fit for the data.

# 4. Select Models
To evaluate the performance of the models, let's look at the R-squared, Mean Squared Error, and Root Mean Squared Error. 
```{r}
models <- list(model1, model2, model3,model4)  
names(models) <- c("Model 1", "Model 2", "Model 3", "Model 4")  

# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Evaluate the models
for (name in names(models)) {
  model <- models[[name]]
  predictions <- predict(model, newdata = df)
  actual_values <- df$TARGET_WINS  
  
  cat(paste("\n", name, "\n"))
  cat(paste("Adjusted R-squared: ", summary(model)$adj.r.squared, "\n"))
  cat(paste("MSE: ", mean((predictions - actual_values)^2), "\n"))
  cat(paste("RMSE: ", rmse(actual_values, predictions), "\n"))
}
```

Based on all of our multiple linear regression models, model 3 seems to have performed best. In comparison to the other models, model 3 has the highest R-squared value at 0.9724, the lowest MSE at 186.97, and the lowest RMSE at 13.67. In addition, the p-value for the coefficient estimates and for the f-statistic were all statistically significant. Also, we did take into account whether the models made practical sense. We concluded that all the models seem to have one or more variables that seem counterinituitive and they all have residual plots that suggest a linear model might not be the best fit for the data. Thus, we decided to go with the model that the highest R-squared and lowest RMSE, which was model 3.  

#### Predicting the evaluation data set
We will use our trained model to predict the number of wins in the evaluation dataset. Here is a summary of the evaluation dataset:
```{r warning=FALSE}
train <- read_csv("https://raw.githubusercontent.com/LeJQC/DATA-621-Group-2/main/HW1/moneyball-evaluation-data.csv", show_col_types = FALSE)

rownames(train) <- train$INDEX

train$INDEX <- NULL

# Deleting HBP column as it has too much missing data
train <- train %>% select(-TEAM_BATTING_HBP)

na_variables <- c("TEAM_BATTING_SO", "TEAM_BASERUN_SB", "TEAM_BASERUN_CS", "TEAM_PITCHING_SO", "TEAM_FIELDING_DP")

# Setting the missing values to the median
for (col in na_variables) {
  median_value <- median(train[[col]], na.rm = TRUE)
  train[[col]][is.na(train[[col]])] <- median_value
}

summary(train)
```

```{r warning=FALSE}
# Looking at the predicts based on our model
predictions <- predict(model3, newdata = train)
actual_values <- df$TARGET_WINS


mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)


print("Model 3: Evaluation data")
print(paste("Mean Squared Error: ", mse))
print(paste("Root Mean Squared Error: ", rmse))
```
When model 3 was applied to the evaluation data, the RSME increased from 13.67 to 18.17, which suggests that the model's predictions are not as accurate on the evaluation data as they were on the training data. The high R-squared value of the model on the training data is an indication that there is possible overfitting. This means that the model is good at predicting data from the training dataset but not unseen data in the evaluation set. In conclusion, more work needs to be done so that the model performs well on both seen and unseen data.   


# Appendix: Code for this assignment

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
